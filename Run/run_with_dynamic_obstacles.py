import tensorflow as tf

# å¼ºåˆ¶å¯ç”¨ TensorFlow 2.x Eager Execution
if hasattr(tf, 'config') and hasattr(tf.config, 'run_functions_eagerly'):
    tf.config.run_functions_eagerly(True)

# ç¦ç”¨ TF 1.x å…¼å®¹æ¨¡å¼ï¼ˆå¦‚æœä¹‹å‰å¯ç”¨äº†ï¼‰
# æ³¨é‡Šæ‰ä»»ä½• tf.compat.v1.disable_eager_execution() è°ƒç”¨

print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
print(f"Eager Execution: {tf.executing_eagerly()}")

import common
import options
import sys
from Run.data import DataPath, DataArgs
import Algo
import Scene  # ä½¿ç”¨åŸå§‹Sceneæ¨¡å—
from Run.exec import Train, Predict, Exec  # æ·»åŠ  Exec
import numpy as np
import math


# å¯¼å…¥åŠ¨æ€éšœç¢ç‰©ç›¸å…³ç±»
from Env.flight_with_dynamic_obstacles import Flight, DynamicObstacle, Scenairo

__agent = None


def default_agent(env, algo_t=options.dqn, model_t=options.M_CNN, force_new=False):
    """ç®€åŒ–ä½¿ç”¨éš¾åº¦ï¼Œè®¾è®¡å¾—ä¸å¤Ÿå¥½"""
    global __agent
    if __agent is None or force_new:  # â† æ·»åŠ  force_new å‚æ•°
        algo = Algo.AlgoDispatch(algo_t, model_t)
        supervisor = algo(buffer_size=500)

        # æ ¹æ®ç®—æ³•ç±»å‹å¤„ç†åŠ¨ä½œç©ºé—´
        if algo_t.upper() == 'DDPG':
            action_dim = 1
            model = supervisor(env.d_states, action_dim, critic_lr=0.001)
        else:
            model = supervisor(env.d_states, env.action.n_actions, critic_lr=0.001)

        __agent = model(size_splits=env.d_states_detail, actor_n_layers=(20, 10),
                        critic_n_layers=(20, 20, 20), n_filters=5, state_n_layers=(20,))
    return __agent

def create_dynamic_obstacles_by_type(dynamic_type='basic'):
    """æ ¹æ®ç±»å‹åˆ›å»ºåŠ¨æ€éšœç¢ç‰© - æ”¯æŒéšæœºæ–¹å‘è¿åŠ¨"""
    if dynamic_type == 'basic':
        return [
            DynamicObstacle([150, 500], 30, 2.0, None, [50, 50, 650, 650], 0.03),
            DynamicObstacle([550, 350], 25, 1.5, None, [50, 50, 650, 650], 0.05),
            DynamicObstacle([300, 100], 35, 1.8, None, [50, 50, 650, 650], 0.02)
        ]
    elif dynamic_type == 'fast':
        return [
            DynamicObstacle([200, 200], 25, 4.0, None, [50, 50, 650, 650], 0.08),
            DynamicObstacle([400, 400], 30, 3.5, None, [50, 50, 650, 650], 0.06),
            DynamicObstacle([500, 200], 20, 5.0, None, [50, 50, 650, 650], 0.10)
        ]
    elif dynamic_type == 'slow_large':
        return [
            DynamicObstacle([150, 300], 50, 1.0, None, [50, 50, 650, 650], 0.02),
            DynamicObstacle([450, 500], 45, 0.8, None, [50, 50, 650, 650], 0.01),
            DynamicObstacle([350, 150], 40, 1.2, None, [50, 50, 650, 650], 0.03)
        ]
    elif dynamic_type == 'mixed':
        return [
            DynamicObstacle([100, 100], 20, 3.0, None, [50, 50, 650, 650], 0.07),  # å¿«å°
            DynamicObstacle([300, 400], 40, 1.5, None, [50, 50, 650, 650], 0.03),  # æ…¢å¤§
            DynamicObstacle([550, 250], 25, 2.5, None, [50, 50, 650, 650], 0.05),  # ä¸­ç­‰
            DynamicObstacle([450, 550], 35, 1.8, None, [50, 50, 650, 650], 0.04),  # æ…¢å¤§
            DynamicObstacle([200, 500], 15, 4.0, None, [50, 50, 650, 650], 0.09)  # å¿«å°
        ]
    elif dynamic_type == 'chaotic':
        # ä½¿ç”¨æ··ä¹±æ¨¡å¼
        from Env.flight_with_dynamic_obstacles import create_chaotic_dynamic_obstacles
        return create_chaotic_dynamic_obstacles()
    elif dynamic_type == 'random':
        # å®Œå…¨éšæœºç”Ÿæˆ
        from Env.flight_with_dynamic_obstacles import create_random_dynamic_obstacles
        return create_random_dynamic_obstacles(num_obstacles=np.random.randint(3, 7))
    else:
        print(f"æœªçŸ¥çš„åŠ¨æ€éšœç¢ç‰©ç±»å‹: {dynamic_type}ï¼Œä½¿ç”¨basicç±»å‹")
        return create_dynamic_obstacles_by_type('basic')


def convert_to_dynamic_scenarios(original_scenarios, dynamic_type='basic', seed=None):
    """å°†åŸå§‹åœºæ™¯è½¬æ¢ä¸ºå¸¦åŠ¨æ€éšœç¢ç‰©çš„åœºæ™¯"""
    dynamic_scenarios = []

    print(f"æ­£åœ¨ä¸º {len(original_scenarios)} ä¸ªåœºæ™¯æ·»åŠ åŠ¨æ€éšœç¢ç‰©")

    # å¦‚æœæä¾›äº†ç§å­ï¼Œè®¾ç½®éšæœºç§å­ä»¥è·å¾—å¯é‡å¤çš„ç»“æœ
    if seed is not None:
        np.random.seed(seed)
        print(f"  ä½¿ç”¨éšæœºç§å­: {seed}")

    for i, orig_scenario in enumerate(original_scenarios):
        # ä¸ºæ¯ä¸ªåœºæ™¯åˆ›å»ºç‹¬ç«‹çš„åŠ¨æ€éšœç¢ç‰©
        # ä½¿ç”¨åœºæ™¯ç´¢å¼•ä½œä¸ºå±€éƒ¨ç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
        if seed is not None:
            np.random.seed(seed + i)

        enhanced_scenario = Scenairo(
            init_pos=orig_scenario.init_pos,
            init_dir=orig_scenario.init_dir,
            goal_pos=orig_scenario.goal_pos,
            goal_dir=orig_scenario.goal_dir,
            circle_obstacles=orig_scenario.circle_obstacles,
            line_obstacles=getattr(orig_scenario, 'line_obstacles', None),
            dynamic_obstacles=create_dynamic_obstacles_by_type(dynamic_type)
        )
        dynamic_scenarios.append(enhanced_scenario)

    return dynamic_scenarios


def training(env, agent, paths, train_scenes, predict_scenes):
    """è®­ç»ƒå‡½æ•° - ä½¿ç”¨å¸¦è¿›åº¦æ¡çš„ Exec ç±»"""
    print("\n=== åŠ¨æ€éšœç¢ç‰©ç¯å¢ƒè®­ç»ƒå¼€å§‹ ===")
    print(f"è®­ç»ƒåœºæ™¯æ•°é‡: {len(train_scenes)}")
    print(f"é¢„æµ‹åœºæ™¯æ•°é‡: {len(predict_scenes)}")

    if train_scenes and hasattr(train_scenes[0], 'dynamic_obstacles'):
        print(f"æ¯ä¸ªåœºæ™¯çš„åŠ¨æ€éšœç¢ç‰©æ•°é‡: {len(train_scenes[0].dynamic_obstacles)}")

    print("\nğŸ”§ æ„å»ºæ¨¡å‹...")
    if hasattr(agent, 'build_model'):
        agent.build_model()
    elif hasattr(agent, 'model') and hasattr(agent.model, 'build_model'):
        agent.model.build_model()
    else:
        print("  âš ï¸ æ‰¾ä¸åˆ° build_model æ–¹æ³•ï¼Œå°è¯•ç»§ç»­...")

    for i in range(args.nth_rounds):
        print(f'\n{"=" * 70}')
        print(f'  è®­ç»ƒå¤§è½®æ¬¡: {i + 1}/{args.nth_rounds}')
        print(f'{"=" * 70}')

        # è®­ç»ƒ
        train_exec = Exec(
            env=env,
            agent=agent,
            scenes=train_scenes,
            paths=paths,
            train=True,
            episodes=args.episodes,
            max_n_step=args.max_n_step
        )
        train_exec()



        # æµ‹è¯•Aï¼šè®­ç»ƒåç«‹å³æµ‹è¯•
        print(f'\nğŸ”¬ æµ‹è¯•Aï¼šè®­ç»ƒåç«‹å³æµ‹è¯•ï¼ˆä¸ä¿å­˜ä¸åŠ è½½ï¼Œä½¿ç”¨å†…å­˜ä¸­çš„æ¨¡å‹ï¼‰...')

        # æµ‹è¯•æ¨¡å‹è¾“å‡º
        print("\næ£€æŸ¥æ¨¡å‹è¾“å‡º...")
        import tensorflow as tf
        test_obs = np.zeros(env.d_states, dtype=np.float32)
        action_probs = agent.model.action(np.expand_dims(test_obs, axis=0))

        if isinstance(action_probs, tf.Tensor):
            action_probs = action_probs.numpy()

        max_prob = np.max(action_probs)
        entropy = -np.sum(action_probs * np.log(action_probs + 1e-10))
        print(f"  è®­ç»ƒå - MaxProb: {max_prob:.4f}, Entropy: {entropy:.4f}")

        test_before_save = Exec(
            env=env,
            agent=agent,
            scenes=train_scenes,
            paths=paths,
            train=False,
            episodes=1,
            max_n_step=args.max_n_step
        )
        test_before_save()

        # ä¿å­˜æ¨¡å‹
        print(f'\nğŸ’¾ ä¿å­˜æ¨¡å‹...')
        save_path = paths.model_save_file()
        print(f"  ä¿å­˜åˆ°: {save_path}")
        agent.model.save_model(save_path)

        # æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶
        import os
        if os.path.exists(save_path):
            print(f"  âœ“ æ–‡ä»¶å·²ä¿å­˜ï¼Œå¤§å°: {os.path.getsize(save_path) / 1024:.2f} KB")

        # æµ‹è¯•Bï¼šåŠ è½½æ¨¡å‹åæµ‹è¯•
        print(f'\nğŸ”¬ æµ‹è¯•Bï¼šåŠ è½½æ¨¡å‹åæµ‹è¯•...')
        agent.model.load_model(save_path)

        # å†æ¬¡æµ‹è¯•æ¨¡å‹è¾“å‡º
        action_probs = agent.model.action(np.expand_dims(test_obs, axis=0))
        if isinstance(action_probs, tf.Tensor):
            action_probs = action_probs.numpy()

        max_prob_after = np.max(action_probs)
        entropy_after = -np.sum(action_probs * np.log(action_probs + 1e-10))
        print(f"  åŠ è½½å - MaxProb: {max_prob_after:.4f}, Entropy: {entropy_after:.4f}")

        if abs(max_prob - max_prob_after) > 0.1:
            print("  âš ï¸ è­¦å‘Šï¼šåŠ è½½å‰åæ¨¡å‹è¾“å‡ºå·®å¼‚å¾ˆå¤§ï¼ä¿å­˜/åŠ è½½æœ‰é—®é¢˜ï¼")
        else:
            print("  âœ“ æ¨¡å‹åŠ è½½æ­£å¸¸")

        test_after_load = Exec(
            env=env,
            agent=agent,
            scenes=train_scenes,
            paths=paths,
            train=False,
            episodes=1,
            max_n_step=args.max_n_step
        )
        test_after_load()

        # åœ¨æµ‹è¯•åœºæ™¯ä¸Šæµ‹è¯•
        print(f'\nğŸ“Š åœ¨æµ‹è¯•åœºæ™¯ä¸Šæµ‹è¯•...')
        predict_exec = Exec(
            env=env,
            agent=agent,
            scenes=predict_scenes,
            paths=paths,
            train=False,
            episodes=1,
            max_n_step=args.max_n_step
        )
        predict_exec()

    print("\nâœ… è®­ç»ƒç»“æŸ")


def prediction(env, agent, paths, predict_scenes):
    """é¢„æµ‹å‡½æ•° - TensorFlow 2.x ç‰ˆæœ¬"""
    print("\n=== åŠ¨æ€éšœç¢ç‰©ç¯å¢ƒé¢„æµ‹å¼€å§‹ ===")

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = paths.model_load_file()
    print(f"\nå°è¯•åŠ è½½æ¨¡å‹: {model_path}")

    import os
    import tensorflow as tf

    if not os.path.exists(model_path):
        print("âœ— æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼")
        return

    print("âœ“ æ¨¡å‹æ–‡ä»¶å­˜åœ¨")
    print(f"  æ–‡ä»¶å¤§å°: {os.path.getsize(model_path) / 1024:.2f} KB")

    # === æ”¹åŠ¨ 1ï¼šåŠ è½½å‰å…ˆæ„å»ºï¼ˆæˆ–ç”¨å‡å‰å‘è§¦å‘å˜é‡åˆ›å»ºï¼‰ ===
    print("\nğŸ”§ æ„å»ºæ¨ç†æ¨¡å‹...")
    try:
        import numpy as np
        # ä¼˜å…ˆè°ƒç”¨æ˜¾å¼çš„ build æ¥å£
        if hasattr(agent, 'build_model'):
            agent.build_model()
        elif hasattr(agent, 'model') and hasattr(agent.model, 'build_model'):
            agent.model.build_model()
        else:
            # å…œåº•ï¼šç”¨ä¸€æ¬¡å‡å‰å‘è§¦å‘å˜é‡åˆ›å»º
            dummy_state = np.zeros((1,) + tuple(env.d_states), dtype=np.float32)
            _ = agent.model.action(dummy_state)
        print("âœ“ æ¨¡å‹ç»“æ„å·²æ„å»º")
    except Exception as e:
        print(f"  âš ï¸ æ„å»ºæ¨¡å‹æ—¶å‡ºé”™ï¼š{e}ï¼Œå°è¯•ç”¨ä¸€æ¬¡å‡å‰å‘è§¦å‘å˜é‡åˆ›å»º")
        try:
            import numpy as np
            dummy_state = np.zeros((1,) + tuple(env.d_states), dtype=np.float32)
            _ = agent.model.action(dummy_state)
            print("âœ“ é€šè¿‡å‡å‰å‘å®Œæˆå˜é‡åˆ›å»º")
        except Exception as ee:
            print(f"  âœ— å‡å‰å‘å¤±è´¥ï¼š{ee}")

    # åŠ è½½æ¨¡å‹ï¼ˆé€šè¿‡ agent.modelï¼Œä¸æ˜¯ agentï¼‰
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹æƒé‡...")
    try:
        # æ­£ç¡®çš„è°ƒç”¨æ–¹å¼
        if hasattr(agent, 'model') and hasattr(agent.model, 'load_model'):
            agent.model.load_model(model_path)
        elif hasattr(agent, 'load_model'):
            agent.load_model(model_path)
        else:
            raise AttributeError("æ‰¾ä¸åˆ° load_model æ¥å£ï¼ˆè¯·æ£€æŸ¥ agent/model å®ç°ï¼‰")
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âœ— åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        traceback.print_exc()
        return

    # æ£€æŸ¥æ¨¡å‹å˜é‡
    print("\nğŸ” æ£€æŸ¥æ¨¡å‹æƒé‡...")
    try:
        variables = []
        if hasattr(agent, 'model') and hasattr(agent.model, 'variables'):
            variables = agent.model.variables
        elif hasattr(agent, 'variables'):
            variables = agent.variables

        print(f"  æ€»å…±æœ‰ {len(variables)} ä¸ªå˜é‡")
        zero_like = True
        for v in variables[:5]:
            v_np = v.numpy() if hasattr(v, 'numpy') else np.array(v)
            if np.any(v_np != 0):
                zero_like = False
                break
        if zero_like:
            print("  âš ï¸ è­¦å‘Šï¼šå˜é‡çœ‹èµ·æ¥åƒæœªåˆå§‹åŒ–æˆ–å…¨ 0")
        else:
            print("  âœ“ å˜é‡éé›¶ï¼ŒåŠ è½½çœ‹èµ·æ¥æ­£å¸¸")
    except Exception as e:
        print(f"  æ— æ³•æ£€æŸ¥å˜é‡ï¼š{e}")

    # å¿«é€Ÿå‰å‘æ£€æŸ¥è¾“å‡ºåˆ†å¸ƒ
    print("\nğŸ”¬ æµ‹è¯•æ¨¡å‹è¾“å‡º...")
    try:
        import numpy as np
        dummy_state = np.zeros((1,) + tuple(env.d_states), dtype=np.float32)

        if hasattr(agent, 'model') and hasattr(agent.model, 'action'):
            action_probs = agent.model.action(dummy_state)
        elif hasattr(agent, 'action'):
            action_probs = agent.action(dummy_state)
        else:
            raise AttributeError("æ²¡æœ‰å‘ç° action æ¥å£ä»¥æ‰§è¡Œä¸€æ¬¡å‰å‘")

        # è½¬æ¢ä¸º numpy
        if isinstance(action_probs, tf.Tensor):
            action_probs = action_probs.numpy()

        # ç¡®ä¿æ˜¯ 2D æ•°ç»„
        if len(action_probs.shape) == 1:
            action_probs = np.expand_dims(action_probs, axis=0)

        max_prob = np.max(action_probs)
        entropy = -np.sum(action_probs * np.log(action_probs + 1e-10))
        top_3_actions = np.argsort(action_probs[0])[-3:][::-1]
        top_3_probs = action_probs[0][top_3_actions]

        print(f"  MaxProb: {max_prob:.4f}, Entropy: {entropy:.4f}")
        print(f"  Top 3 actions: {top_3_actions}, probs: {top_3_probs}")

        if max_prob < 0.2:
            print("  âš ï¸ è­¦å‘Šï¼šæ¨¡å‹è¾“å‡ºæ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼ˆMaxProb < 0.2ï¼‰")
            print("  è¿™å¯èƒ½æ„å‘³ç€æ¨¡å‹æ²¡æœ‰æ­£ç¡®è®­ç»ƒæˆ–åŠ è½½ï¼")
        elif entropy > 2.0:
            print("  âš ï¸ è­¦å‘Šï¼šç†µå€¼è¿‡é«˜ï¼ˆEntropy > 2.0ï¼‰")
        else:
            print("  âœ“ æ¨¡å‹è¾“å‡ºçœ‹èµ·æ¥æ­£å¸¸")

    except Exception as e:
        print(f"  âš ï¸ æµ‹è¯•æ¨¡å‹è¾“å‡ºæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    # æ­¥éª¤1ï¼šåœ¨è®­ç»ƒåœºæ™¯ä¸Šæµ‹è¯•
    print("\n" + "=" * 70)
    print("  ğŸ” æ­¥éª¤1ï¼šåœ¨è®­ç»ƒåœºæ™¯ä¸Šæµ‹è¯•")
    print("=" * 70)

    train_scenes, train_info = build_train_scenes(args)
    if train_info and 'independent_circle' in train_info:
        print(f"\nåŠ è½½è®­ç»ƒåœºæ™¯...")
        print(f"independent_circle: task_size: {train_info['independent_circle']['task_size']}, "
              f"obstacles_size: {train_info['independent_circle']['obstacles_size']}")
    if train_info and 'obs_id' in train_info:
        print(f"obs_id: {train_info['obs_id']}, task_id: {train_info.get('task_id')}, "
              f"n_scenarios: {train_info.get('n_scenarios')}, percentage: {train_info.get('percentage')}")
    print(f"æ­£åœ¨ä¸º {len(train_scenes)} ä¸ªåœºæ™¯æ·»åŠ åŠ¨æ€éšœç¢ç‰©")
    print(f"  ä½¿ç”¨éšæœºç§å­: {getattr(args, 'nth_times', -1)}")

    # ä¸é¢„æµ‹åœºæ™¯ä¿æŒç›¸åŒçš„åŠ¨æ€éšœç¢ç‰©è®¾å®š
    dyns = create_dynamic_obstacles_by_type(getattr(args, "dynamic_obstacle_type", "basic"))
    for sc in train_scenes:
        sc.dynamic_obstacles = dyns

    print(f"è®­ç»ƒåœºæ™¯æ•°é‡: {len(train_scenes)}")

    test_on_train_exec = Exec(
        env=env,
        agent=agent,
        scenes=train_scenes,
        paths=paths,
        train=False,
        episodes=1,
        max_n_step=args.max_n_step
    )
    test_on_train_exec()

    # æ­¥éª¤2ï¼šåœ¨æµ‹è¯•åœºæ™¯ä¸Šæµ‹è¯•
    print("\n" + "=" * 70)
    print("  ğŸ“Š æ­¥éª¤2ï¼šåœ¨æµ‹è¯•åœºæ™¯ä¸Šæµ‹è¯•")
    print("=" * 70)
    print(f"æµ‹è¯•åœºæ™¯æ•°é‡: {len(predict_scenes)}")

    if predict_scenes and hasattr(predict_scenes[0], 'dynamic_obstacles'):
        print(f"æ¯ä¸ªåœºæ™¯çš„åŠ¨æ€éšœç¢ç‰©æ•°é‡: {len(predict_scenes[0].dynamic_obstacles)}")

    predict_exec = Exec(
        env=env,
        agent=agent,
        scenes=predict_scenes,
        paths=paths,
        train=False,
        episodes=1,
        max_n_step=args.max_n_step
    )
    predict_exec()

    print("\nâœ… é¢„æµ‹ç»“æŸ")



def run(args):
    """ä¸»è¿è¡Œå‡½æ•° - æ”¯æŒåŠ¨æ€éšœç¢ç‰©"""
    print(f"\nåŠ¨æ€éšœç¢ç‰©ç±»å‹: {args.dynamic_obstacle_type}")
    print(f"éšœç¢ç‰©ç±»å‹: {args.obstacle_type}")
    print(f"ç®—æ³•ç±»å‹: {args.algo_t}")

    # ä½¿ç”¨åŸå§‹çš„Sceneæ¨¡å—åŠ è½½åœºæ™¯
    if args.obstacle_type == options.O_circle:
        train_task = Scene.circle_train_task4x200
        test_task = Scene.circle_test_task4x100
    else:
        train_task = Scene.line_train_task4x200
        test_task = Scene.line_test_task4x100

    scene = Scene.ScenarioLoader()
    print("\nåŠ è½½åŸå§‹è®­ç»ƒåœºæ™¯: ")
    original_train_scenes = scene.load_scene(**train_task, percentage=args.percentage)
    print("\nåŠ è½½åŸå§‹é¢„æµ‹åœºæ™¯: ")
    original_predict_scenes = scene.load_scene(**test_task)

    # è½¬æ¢ä¸ºå¸¦åŠ¨æ€éšœç¢ç‰©çš„åœºæ™¯- ä½¿ç”¨å›ºå®šç§å­
    print(f"\nä¸ºåœºæ™¯æ·»åŠ åŠ¨æ€éšœç¢ç‰© (ç±»å‹: {args.dynamic_obstacle_type})...")

    # ä½¿ç”¨ nth_times ä½œä¸ºç§å­ï¼Œç¡®ä¿ç›¸åŒç¼–å·çš„å®éªŒä½¿ç”¨ç›¸åŒçš„åŠ¨æ€éšœç¢ç‰©
    seed = args.nth_times if args.nth_times >= 0 else 42
    print(f"ä½¿ç”¨ç§å­ {seed} ç”ŸæˆåŠ¨æ€éšœç¢ç‰©")

    train_scenes = convert_to_dynamic_scenarios(original_train_scenes, args.dynamic_obstacle_type, seed=seed)
    predict_scenes = convert_to_dynamic_scenarios(original_predict_scenes, args.dynamic_obstacle_type, seed=seed + 10000)

    print(f"å¢å¼ºåè®­ç»ƒåœºæ™¯æ•°é‡: {len(train_scenes)}")
    print(f"å¢å¼ºåé¢„æµ‹åœºæ™¯æ•°é‡: {len(predict_scenes)}")

    # æ ¹æ®ç®—æ³•ç±»å‹åˆ›å»ºç¯å¢ƒ
    if args.algo_t.upper() == 'DDPG':
        print("ä½¿ç”¨è¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆDDPGç®—æ³•ï¼‰")
        from Env.flight_with_dynamic_obstacles import ContinueAction
        action = ContinueAction(45, 10., math.pi / 60, 4)
        env = Flight(action=action)
    else:
        print("ä½¿ç”¨ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆDQNç­‰ç®—æ³•ï¼‰")
        from Env.flight_with_dynamic_obstacles import DiscreteAction
        action = DiscreteAction(45, 10., math.pi / 60, 4, 5)
        env = Flight(action=action)

    # æµ‹è¯•æ¨¡å¼ä¸‹å¼ºåˆ¶åˆ›å»ºæ–°çš„ agent
    if not args.train:
        agent = default_agent(env, args.algo_t, args.model_t, force_new=True)  # â† æ·»åŠ  force_new=True
    else:
        agent = default_agent(env, args.algo_t, args.model_t)

    paths = DataPath(args)

    if args.train:
        training(env, agent, paths, train_scenes, predict_scenes)
    else:
        prediction(env, agent, paths, predict_scenes)


def demo_single_scenario():
    """æ¼”ç¤ºå•ä¸ªåŠ¨æ€éšœç¢ç‰©åœºæ™¯"""
    print("\n=== å•åœºæ™¯æ¼”ç¤ºæ¨¡å¼ ===")

    # åˆ›å»ºç®€å•çš„æµ‹è¯•åœºæ™¯
    dynamic_obstacles = create_dynamic_obstacles_by_type('basic')
    scenario = Scenairo(
        init_pos=[100, 100],
        init_dir=45,
        goal_pos=[600, 600],
        goal_dir=0,
        circle_obstacles=[[300, 300, 40], [450, 200, 35]],
        dynamic_obstacles=dynamic_obstacles
    )

    env = Flight()
    state = env.reset(scenario)

    print("å¼€å§‹æ¼”ç¤º...")
    print(f"èµ·å§‹ä½ç½®: {scenario.init_pos}")
    print(f"ç›®æ ‡ä½ç½®: {scenario.goal_pos}")
    print(f"åŠ¨æ€éšœç¢ç‰©æ•°é‡: {len(scenario.dynamic_obstacles)}")

    # ç®€å•çš„è·¯å¾„è§„åˆ’æ¼”ç¤º
    for step in range(100):
        # ä½¿ç”¨ç®€å•çš„ç­–ç•¥ï¼šä¸»è¦å‘ç›®æ ‡å‰è¿›ï¼Œå¶å°”è½¬å‘
        if step % 10 == 0:
            action = np.random.choice([8, 12, 16])  # å·¦è½¬ã€ç›´è¡Œã€å³è½¬
        else:
            action = 12  # ä¸»è¦ç›´è¡Œ

        if action >= env.action.n_actions:
            action = action % env.action.n_actions

        state, reward, done = env.step(action)
        env.render(sleep_time=0.1, show_trace=True, show_pos=True, show_arrow=True)

        if step % 20 == 0:
            print(f"æ­¥éª¤ {step}: ä½ç½®=({env.current_pos[0]:.1f}, {env.current_pos[1]:.1f}), å¥–åŠ±={reward:.3f}")

        if done:
            print(f"ä»»åŠ¡å®Œæˆäºæ­¥éª¤ {step}!")
            result_names = ['è¶…æ—¶', 'å¤±è´¥', 'è¾¹ç•Œç¢°æ’', 'æˆåŠŸ']
            print(f"ç»“æœ: {result_names[env.result]}")
            break

    input("æŒ‰å›è½¦é”®ç»“æŸæ¼”ç¤º...")


if __name__ == "__main__":
    # åŠ¨æ€éšœç¢ç‰©ç›¸å…³å‚æ•°
    args = DataArgs(train=False, nth_times=25)  # è®¾ç½®ä¸ºFalseè¿›è¡Œé¢„æµ‹æµ‹è¯• True,False
    args.max_n_step = 200

    # æŒ‡å®šå…·ä½“çš„ç®—æ³•å’Œæ¨¡å‹
    args.algo_t = 'AC'  # æˆ– 'DDPG', 'DQN', ç­‰
    args.model_t = 'cnn'  # æˆ– 'mlp', 'cnn', ç­‰
    args.env_t = 'flight'

    # åŠ¨æ€éšœç¢ç‰©ç±»å‹é€‰æ‹© - æ–°å¢éšæœºè¿åŠ¨æ¨¡å¼
    # å¯é€‰: 'basic', 'fast', 'slow_large', 'mixed', 'chaotic', 'random'
    args.dynamic_obstacle_type = 'basic'  # ä½¿ç”¨æ··ä¹±æ¨¡å¼æµ‹è¯•éšæœºè¿åŠ¨

    """for prediction"""
    args.draw_rate = 0.1

    """for training"""
    args.episodes = 1
    args.nth_rounds = 1
    args.percentage = 0.3

    print("åŠ¨æ€éšœç¢ç‰©ç¯å¢ƒé…ç½®:")
    print(args.__dict__)
    print(f"\nåŠ¨æ€éšœç¢ç‰©è¿åŠ¨æ¨¡å¼è¯´æ˜:")
    print(f"- basic: åŸºç¡€éšæœºè¿åŠ¨ï¼Œé€‚ä¸­çš„æ–¹å‘å˜åŒ–")
    print(f"- fast: å¿«é€Ÿéšæœºè¿åŠ¨ï¼Œé¢‘ç¹çš„æ–¹å‘å˜åŒ–")
    print(f"- slow_large: æ…¢é€Ÿå¤§å‹éšœç¢ç‰©ï¼Œè¾ƒå°‘æ–¹å‘å˜åŒ–")
    print(f"- mixed: æ··åˆé€Ÿåº¦å’Œå¤§å°ï¼Œä¸åŒå˜åŒ–é¢‘ç‡")
    print(f"- chaotic: æ··ä¹±æ¨¡å¼ï¼Œé«˜åº¦ä¸å¯é¢„æµ‹")
    print(f"- random: å®Œå…¨éšæœºç”Ÿæˆçš„éšœç¢ç‰©")
    print(f"\nå½“å‰ä½¿ç”¨æ¨¡å¼: {args.dynamic_obstacle_type}")

    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == 'demo':
        # æ¼”ç¤ºæ¨¡å¼
        demo_single_scenario()
    else:
        # æ­£å¸¸è®­ç»ƒ/é¢„æµ‹æ¨¡å¼
        run(args)